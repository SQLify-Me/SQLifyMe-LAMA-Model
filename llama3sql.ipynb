{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":33146,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":27739},{"sourceId":33551,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28083}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-23T18:00:55.557041Z","iopub.execute_input":"2024-06-23T18:00:55.557967Z","iopub.status.idle":"2024-06-23T18:00:56.413251Z","shell.execute_reply.started":"2024-06-23T18:00:55.557929Z","shell.execute_reply":"2024-06-23T18:00:56.412182Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3/pytorch/8b-chat/1/consolidated.00.pth\n/kaggle/input/llama-3/pytorch/8b-chat/1/LICENSE\n/kaggle/input/llama-3/pytorch/8b-chat/1/MODEL_CARD.md\n/kaggle/input/llama-3/pytorch/8b-chat/1/params.json\n/kaggle/input/llama-3/pytorch/8b-chat/1/model.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/USE_POLICY.md\n/kaggle/input/llama-3/pytorch/8b-chat/1/Llama3_Repo.jpeg\n/kaggle/input/llama-3/pytorch/8b-chat/1/example_text_completion.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/test_tokenizer.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/requirements.txt\n/kaggle/input/llama-3/pytorch/8b-chat/1/tokenizer.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/eval_details.md\n/kaggle/input/llama-3/pytorch/8b-chat/1/generation.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/__init__.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/example_chat_completion.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/tokenizer.model\n/kaggle/input/llama-3/pytorch/8b-chat/1/setup.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input/llama-3/pytorch/8b-chat/1'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:00:56.414702Z","iopub.execute_input":"2024-06-23T18:00:56.415069Z","iopub.status.idle":"2024-06-23T18:00:56.420947Z","shell.execute_reply.started":"2024-06-23T18:00:56.415044Z","shell.execute_reply":"2024-06-23T18:00:56.420021Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3/pytorch/8b-chat/1/consolidated.00.pth\n/kaggle/input/llama-3/pytorch/8b-chat/1/LICENSE\n/kaggle/input/llama-3/pytorch/8b-chat/1/MODEL_CARD.md\n/kaggle/input/llama-3/pytorch/8b-chat/1/params.json\n/kaggle/input/llama-3/pytorch/8b-chat/1/model.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/USE_POLICY.md\n/kaggle/input/llama-3/pytorch/8b-chat/1/Llama3_Repo.jpeg\n/kaggle/input/llama-3/pytorch/8b-chat/1/example_text_completion.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/test_tokenizer.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/requirements.txt\n/kaggle/input/llama-3/pytorch/8b-chat/1/tokenizer.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/eval_details.md\n/kaggle/input/llama-3/pytorch/8b-chat/1/generation.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/__init__.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/example_chat_completion.py\n/kaggle/input/llama-3/pytorch/8b-chat/1/tokenizer.model\n/kaggle/input/llama-3/pytorch/8b-chat/1/setup.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n%pip install -U langchain  langchain_community langchain-huggingface transformers accelerate","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:03:19.452157Z","iopub.execute_input":"2024-06-23T18:03:19.453023Z","iopub.status.idle":"2024-06-23T18:03:40.576724Z","shell.execute_reply.started":"2024-06-23T18:03:19.452991Z","shell.execute_reply":"2024-06-23T18:03:40.575511Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM,pipeline\nfrom langchain_huggingface import HuggingFacePipeline","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:03:58.134872Z","iopub.execute_input":"2024-06-23T18:03:58.135251Z","iopub.status.idle":"2024-06-23T18:04:03.896811Z","shell.execute_reply.started":"2024-06-23T18:03:58.135218Z","shell.execute_reply":"2024-06-23T18:04:03.895961Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nbase_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:08:12.116216Z","iopub.execute_input":"2024-06-23T18:08:12.117131Z","iopub.status.idle":"2024-06-23T18:10:05.703294Z","shell.execute_reply.started":"2024-06-23T18:08:12.117097Z","shell.execute_reply":"2024-06-23T18:10:05.702327Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"656e9856ef7a41ccb441f9a94707adf7"}},"metadata":{}}]},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer,\n    max_new_tokens=120\n)\n\nllm = HuggingFacePipeline(pipeline=pipe)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:12:46.456474Z","iopub.execute_input":"2024-06-23T18:12:46.456884Z","iopub.status.idle":"2024-06-23T18:12:46.468244Z","shell.execute_reply.started":"2024-06-23T18:12:46.456852Z","shell.execute_reply":"2024-06-23T18:12:46.467276Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"result = llm.invoke(\n    \"Explain the difference between ChatGPT and open source LLMs in a couple of lines.\"\n)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:13:26.727962Z","iopub.execute_input":"2024-06-23T18:13:26.728357Z","iopub.status.idle":"2024-06-23T18:13:35.813541Z","shell.execute_reply.started":"2024-06-23T18:13:26.728308Z","shell.execute_reply":"2024-06-23T18:13:35.812496Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Explain the difference between ChatGPT and open source LLMs in a couple of lines.}\n\nChatGPT is a proprietary AI model developed by Meta AI that is trained on a massive dataset and fine-tuned for conversational dialogue. Open source LLMs, on the other hand, are publicly available AI models that are trained on large datasets and can be modified and fine-tuned by anyone, often for specific tasks or domains.\n\n{Explain the difference between ChatGPT and open source LLMs in a couple of lines.}\n\nChatGPT is a proprietary AI model developed by Meta AI that is trained on a massive dataset and fine-tuned for conversational dialogue. Open\n","output_type":"stream"}]},{"cell_type":"code","source":"result = llm.invoke(\n    \"Retrieve all products.\"\n)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:14:05.318391Z","iopub.execute_input":"2024-06-23T18:14:05.318749Z","iopub.status.idle":"2024-06-23T18:14:13.399304Z","shell.execute_reply.started":"2024-06-23T18:14:05.318720Z","shell.execute_reply":"2024-06-23T18:14:13.398306Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Retrieve all products. The query should return all products, including their names, descriptions, and prices.\n\n```\nSELECT * FROM products;\n```\n\n2. Retrieve all products with a price greater than $50. The query should return all products with a price greater than $50, including their names, descriptions, and prices.\n\n```\nSELECT * FROM products WHERE price > 50;\n```\n\n3. Retrieve all products with a name containing the word \"phone\". The query should return all products with a name containing the word \"phone\", including their names, descriptions, and prices.\n\n```\nSELECT * FROM products WHERE\n","output_type":"stream"}]},{"cell_type":"code","source":"result = llm.invoke(\n    \"Write SQL to retrieve all products.\"\n)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:14:20.250057Z","iopub.execute_input":"2024-06-23T18:14:20.250753Z","iopub.status.idle":"2024-06-23T18:14:28.347408Z","shell.execute_reply.started":"2024-06-23T18:14:20.250718Z","shell.execute_reply":"2024-06-23T18:14:28.346431Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Write SQL to retrieve all products. The SQL query should include the following conditions:\n    - Retrieve all products that have a price greater than or equal to $50.\n    - Retrieve all products that have a category of \"Electronics\" or \"Home Appliances\".\n    - Retrieve all products that have a rating of 4 or 5.\n\nHere is the SQL query that meets the conditions:\n```sql\nSELECT *\nFROM products\nWHERE price >= 50\nAND (category = 'Electronics' OR category = 'Home Appliances')\nAND (rating = 4 OR rating = 5);\n```\nThis SQL query will retrieve all\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.chains import LLMChain\nfrom langchain import PromptTemplate\nimport re\n","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:25:31.614806Z","iopub.execute_input":"2024-06-23T18:25:31.615445Z","iopub.status.idle":"2024-06-23T18:25:31.619823Z","shell.execute_reply.started":"2024-06-23T18:25:31.615413Z","shell.execute_reply":"2024-06-23T18:25:31.618803Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def getAnswer(question):\n  template = \"\"\"\n  <s>[INST] <<SYS>>\n  Take a deep breathe and work on this step by step.\n  You are a SQL developer that wants to extract information from a database using SQL queries. Your task is to generate SQL queries based on the input natural language. Your queries should follow the SQL standard and be compatible with the company's database management system. Ensure that your queries are structured logically, efficiently, and properly formatted.Do not explain.Provide the SQL query only.\n  It is important for me, so please generate an efficient syntactically correct SQL query.\n  <</SYS>>\n\n\n  {text} [/INST]\n  \"\"\"\n\n\n\n  prompt = PromptTemplate(\n      input_variables=[\"text\"],\n      template=template,\n  )\n\n  chain = LLMChain(llm=llm, prompt=prompt)\n  ans=chain.run(question)\n  sql_query = re.search(r'SELECT[^;]+;', ans, re.DOTALL)\n\n  if sql_query:\n    sql_query = sql_query.group().strip()\n  else:\n    sql_query=\"Please repharse your natural language question\"\n\n  return sql_query","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:25:33.188867Z","iopub.execute_input":"2024-06-23T18:25:33.189717Z","iopub.status.idle":"2024-06-23T18:25:33.196806Z","shell.execute_reply.started":"2024-06-23T18:25:33.189681Z","shell.execute_reply":"2024-06-23T18:25:33.195098Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"question=\"Retrieve all products.\"\nresult = getAnswer(question)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:25:33.772317Z","iopub.execute_input":"2024-06-23T18:25:33.772858Z","iopub.status.idle":"2024-06-23T18:25:42.137241Z","shell.execute_reply.started":"2024-06-23T18:25:33.772828Z","shell.execute_reply":"2024-06-23T18:25:42.136177Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"SELECT * \n   FROM products;\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}